{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU_IF_AVAILABLE = False\n",
    "\n",
    "MODEL_DIR = (\n",
    "    'C:/' +\n",
    "    'Users/' +\n",
    "    'Abdul Zakkar/' +\n",
    "    'Documents/' +\n",
    "    'UICOM/' +\n",
    "    'research/' +\n",
    "    'salahudeen/' +\n",
    "    'models/'\n",
    ")\n",
    "\n",
    "INPUTS = [\n",
    "    'age','race_2','pred_yr1',\n",
    "]\n",
    "\n",
    "OUTPUTS = [\n",
    "    'canc_yr_1','canc_yr_2','canc_yr_3','canc_yr_4','canc_yr_5','canc_yr_6'\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "N_NETWORKS = 100\n",
    "N_YEARS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if USE_GPU_IF_AVAILABLE:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, mean, std, **kwargs):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.weight_layer = nn.Linear(input_size, input_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout', 0.2))\n",
    "\n",
    "        hidden_size = kwargs.get('hidden_size', 128)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.mean = nn.Parameter(torch.tensor(mean, dtype=torch.float32), requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.tensor(std, dtype=torch.float32), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = (x - self.mean) / self.std # Standardize the input\n",
    "\n",
    "        weights = torch.sigmoid(self.weight_layer(x))\n",
    "        x = x * weights\n",
    "        \n",
    "        x = self.dropout(self.relu(self.layer1(x)))\n",
    "        x = self.dropout(self.relu(self.layer2(x)))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function to handle masks\n",
    "class MaskedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedBCELoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, targets, masks):\n",
    "        loss = self.bce_loss(outputs, targets)\n",
    "        loss = loss * masks # Apply mask and weights\n",
    "        return loss.sum() / masks.sum()  # Average loss over present labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, features, labels, masks):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx], self.masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, dataloader, epochs,\n",
    "    report_every_n_epochs=1, verbose=0\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, masks in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if (epoch + 1) % report_every_n_epochs == 0 and verbose >= 1:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        for features, _, _ in dataloader:\n",
    "            outputs = model(features)\n",
    "            all_outputs.append(outputs)\n",
    "        pred_cols = ['pred_yr'+str(i) for i in range(1,N_YEARS+1)]\n",
    "        return pd.DataFrame(np.reshape(torch.cat(all_outputs).cpu().numpy(), (-1, 6)), columns=pred_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = (\n",
    "    'C:/' +\n",
    "    'Users/' +\n",
    "    'Abdul Zakkar/' +\n",
    "    'Documents/' +\n",
    "    'UICOM/' +\n",
    "    'research/' +\n",
    "    'salahudeen/' +\n",
    "    'test_train_sets/' +\n",
    "    'train/' +\n",
    "    'train_nlst_svm7sybil.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_train = df_train[INPUTS + OUTPUTS]\n",
    "df_train = df_train.dropna(subset=['canc_yr_1']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = df_train[INPUTS]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "mean = scaler.mean_\n",
    "std = scaler.scale_\n",
    "\n",
    "features_train = features_train.to_numpy().astype(np.float32)\n",
    "labels_train = df_train[OUTPUTS].fillna(0).to_numpy().astype(np.float32)\n",
    "masks_train = df_train[OUTPUTS].notna().astype(int).to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.tensor(features_train, device=device)\n",
    "labels_train = torch.tensor(labels_train, device=device)\n",
    "masks_train = torch.tensor(masks_train, device=device)\n",
    "\n",
    "dataset_train = MaskedDataset(features_train, labels_train, masks_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1: DONE.\n",
      "Training model 2: DONE.\n",
      "Training model 3: DONE.\n",
      "Training model 4: DONE.\n",
      "Training model 5: DONE.\n",
      "Training model 6: DONE.\n",
      "Training model 7: DONE.\n",
      "Training model 8: DONE.\n",
      "Training model 9: DONE.\n",
      "Training model 10: DONE.\n",
      "Training model 11: DONE.\n",
      "Training model 12: DONE.\n",
      "Training model 13: DONE.\n",
      "Training model 14: DONE.\n",
      "Training model 15: DONE.\n",
      "Training model 16: DONE.\n",
      "Training model 17: DONE.\n",
      "Training model 18: DONE.\n",
      "Training model 19: DONE.\n",
      "Training model 20: DONE.\n",
      "Training model 21: DONE.\n",
      "Training model 22: DONE.\n",
      "Training model 23: DONE.\n",
      "Training model 24: DONE.\n",
      "Training model 25: DONE.\n",
      "Training model 26: DONE.\n",
      "Training model 27: DONE.\n",
      "Training model 28: DONE.\n",
      "Training model 29: DONE.\n",
      "Training model 30: DONE.\n",
      "Training model 31: DONE.\n",
      "Training model 32: DONE.\n",
      "Training model 33: DONE.\n",
      "Training model 34: DONE.\n",
      "Training model 35: DONE.\n",
      "Training model 36: DONE.\n",
      "Training model 37: DONE.\n",
      "Training model 38: DONE.\n",
      "Training model 39: DONE.\n",
      "Training model 40: DONE.\n",
      "Training model 41: DONE.\n",
      "Training model 42: DONE.\n",
      "Training model 43: DONE.\n",
      "Training model 44: DONE.\n",
      "Training model 45: DONE.\n",
      "Training model 46: DONE.\n",
      "Training model 47: DONE.\n",
      "Training model 48: DONE.\n",
      "Training model 49: DONE.\n",
      "Training model 50: DONE.\n",
      "Training model 51: DONE.\n",
      "Training model 52: DONE.\n",
      "Training model 53: DONE.\n",
      "Training model 54: DONE.\n",
      "Training model 55: DONE.\n",
      "Training model 56: DONE.\n",
      "Training model 57: DONE.\n",
      "Training model 58: DONE.\n",
      "Training model 59: DONE.\n",
      "Training model 60: DONE.\n",
      "Training model 61: DONE.\n",
      "Training model 62: DONE.\n",
      "Training model 63: DONE.\n",
      "Training model 64: DONE.\n",
      "Training model 65: DONE.\n",
      "Training model 66: DONE.\n",
      "Training model 67: DONE.\n",
      "Training model 68: DONE.\n",
      "Training model 69: DONE.\n",
      "Training model 70: DONE.\n",
      "Training model 71: DONE.\n",
      "Training model 72: DONE.\n",
      "Training model 73: DONE.\n",
      "Training model 74: DONE.\n",
      "Training model 75: DONE.\n",
      "Training model 76: DONE.\n",
      "Training model 77: DONE.\n",
      "Training model 78: DONE.\n",
      "Training model 79: DONE.\n",
      "Training model 80: DONE.\n",
      "Training model 81: DONE.\n",
      "Training model 82: DONE.\n",
      "Training model 83: DONE.\n",
      "Training model 84: DONE.\n",
      "Training model 85: DONE.\n",
      "Training model 86: DONE.\n",
      "Training model 87: DONE.\n",
      "Training model 88: DONE.\n",
      "Training model 89: DONE.\n",
      "Training model 90: DONE.\n",
      "Training model 91: DONE.\n",
      "Training model 92: DONE.\n",
      "Training model 93: DONE.\n",
      "Training model 94: DONE.\n",
      "Training model 95: DONE.\n",
      "Training model 96: DONE.\n",
      "Training model 97: DONE.\n",
      "Training model 98: DONE.\n",
      "Training model 99: DONE.\n",
      "Training model 100: DONE.\n"
     ]
    }
   ],
   "source": [
    "for network in range(N_NETWORKS):\n",
    "    print(f'Training model {network+1}: ', end='')\n",
    "    model = NeuralNetwork(\n",
    "        len(INPUTS),\n",
    "        len(OUTPUTS),\n",
    "        mean,\n",
    "        std,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    criterion = MaskedBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        dataloader_train,\n",
    "        EPOCHS)\n",
    "    model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "    model_scripted.save(MODEL_DIR + f'nn_{len(INPUTS)}inputs_nlst_uic_{network+1}.pth') # Save\n",
    "    print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE = (\n",
    "    'C:/' +\n",
    "    'Users/' +\n",
    "    'Abdul Zakkar/' +\n",
    "    'Documents/' +\n",
    "    'UICOM/' +\n",
    "    'research/' +\n",
    "    'salahudeen/' +\n",
    "    'test_train_sets/' +\n",
    "    'test/' +\n",
    "    'test_uic_svm7sybil.csv'\n",
    ")\n",
    "\n",
    "RESULTS_FILE = (\n",
    "    'C:/' +\n",
    "    'Users/' +\n",
    "    'Abdul Zakkar/' +\n",
    "    'Documents/' +\n",
    "    'UICOM/' +\n",
    "    'research/' +\n",
    "    'salahudeen/' +\n",
    "    'test_train_sets/' +\n",
    "    'test/' +\n",
    "    'test_uic_nn2s.csv'\n",
    ")\n",
    "\n",
    "INDIVIDUAL_RESULTS_FILE = (\n",
    "    'C:/' +\n",
    "    'Users/' +\n",
    "    'Abdul Zakkar/' +\n",
    "    'Documents/' +\n",
    "    'UICOM/' +\n",
    "    'research/' +\n",
    "    'salahudeen/' +\n",
    "    'test_train_sets/' +\n",
    "    'test_ensemble/' +\n",
    "    'test_uic_nn2s_{x}.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_matching_pattern(directory, pattern):\n",
    "    regex = re.compile(pattern)\n",
    "    output = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if regex.search(file):\n",
    "                output.append(os.path.join(root, file))\n",
    "    return output\n",
    "\n",
    "model_paths = find_files_matching_pattern(MODEL_DIR, r'nn_3inputs_nlst_uic_\\d+\\.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_FILE)\n",
    "df_test = df_test[INPUTS + OUTPUTS]\n",
    "df_test = df_test.dropna(subset=['canc_yr_1']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = df_test[INPUTS].to_numpy().astype(np.float32)\n",
    "labels_test = df_test[OUTPUTS].fillna(0).to_numpy().astype(np.float32)\n",
    "masks_test = df_test[OUTPUTS].notna().astype(int).to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = torch.tensor(features_test, device=device)\n",
    "labels_test = torch.tensor(labels_test, device=device)\n",
    "masks_test = torch.tensor(masks_test, device=device)\n",
    "\n",
    "dataset_test = MaskedDataset(features_test, labels_test, masks_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    model = torch.jit.load(model_path)\n",
    "    result = evaluate_model(model, dataloader_test)\n",
    "    truth = df_test[['canc_yr_'+str(i) for i in range(1,N_YEARS+1)]]\n",
    "    result = pd.concat([result, truth], axis=1)\n",
    "    result.to_csv(INDIVIDUAL_RESULTS_FILE.replace('{x}', str(i+1)), index=False)\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results, keys=range(len(results)))\n",
    "results = results.groupby(level=1).mean()\n",
    "\n",
    "results.to_csv(RESULTS_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
